üìã EXAMEN FINAL - PR√ÅCTICO (Ensambles)

Catedr√°tica: Ing. Nicole Rodr√≠guez Tema: M√©todos de Ensamble (Bagging, Boosting y Stacking)

Contexto del Negocio: Una empresa de Telecomunicaciones est√° perdiendo clientes ("Churn"). Quieren predecir qui√©n se va a ir para ofrecerle una promoci√≥n antes de que sea tarde. Han probado un √Årbol de Decisi√≥n simple y fall√≥. Ahora te piden usar M√©todos de Ensamble para mejorar la precisi√≥n.

Dataset: Descarga el famoso "Telco Customer Churn": üîó Link Kaggle: Telco Customer Churn (Archivo: WA*Fn-UseC*-Telco-Customer-Churn.csv)

Instrucciones T√©cnicas (Nivel Experto):

Desarrolla el script ensemble_battle.py:

    Limpieza y EDA R√°pido:

        Carga datos con argparse.

        OJO: La columna TotalCharges tiene espacios vac√≠os que parecen texto. Convi√©rtela a num√©rico (pd.to_numeric(..., errors='coerce')) y llena los nulos.

        Target: Churn (Yes/No). Convi√©rtelo a 1/0.

        Elimina customerID.

    Preprocessing (Pipeline Robusto):

        Tienes columnas num√©ricas y categ√≥ricas.

        Usa ColumnTransformer:

            Num√©ricas ‚Üí StandardScaler.

            Categ√≥ricas ‚Üí OneHotEncoder.

    La Batalla de Modelos (Entrenamiento): Entrena y compara estos 3 modelos usando el mismo X_train:

        Competidor A (Bagging): RandomForestClassifier

            n_estimators=100, random_state=42.

        Competidor B (Boosting): GradientBoostingClassifier

            n_estimators=100, learning_rate=0.1.

        Competidor C (Stacking): StackingClassifier

            Estimadores base: Un RandomForest y un SVC.

            Estimador final: LogisticRegression.

    Evaluaci√≥n Final:

        Crea un bucle que recorra los 3 modelos.

        Para cada uno imprime: Accuracy y F1-Score.

        ¬øCu√°l gan√≥? (Normalmente Boosting o Stacking ganan por poco).
